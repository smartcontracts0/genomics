{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartcontracts0/genomics/blob/main/FHE/DNAFHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install concrete-ml\n",
        "!pip install faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVw-xZzqY6ED",
        "outputId": "3b2f5a57-f4c9-4321-e647-fe8fa2791706"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              sample_id  \\\n",
            "0  8b7a6e21-2d1d-43f3-8313-19c8f29a42d6   \n",
            "1  18172ece-7f7c-4a6a-9435-e5fff9cf1c13   \n",
            "2  6d0fcdc1-4f61-4076-a89d-afa5eee4b002   \n",
            "3  e45f2aab-1aec-492d-a301-6e523c0a03da   \n",
            "4  42edfef1-12e4-4a27-807a-d032551c00e0   \n",
            "\n",
            "                                            sequence chromosome region_type  \\\n",
            "0  CTACCCTGCTGATCTAGGACGCATTCAACGGCTCTTTCTAAAGCGG...         10      coding   \n",
            "1  GCGGGCGACCATACCAACCATCCTGGGAAACCGCATGCCTACTGTA...          Y      coding   \n",
            "2  AACGACGGGACGTCATTGGGCACCTGGGACACGGCCGCCTTGTGCC...         15  non-coding   \n",
            "3  TCGTGAACACGCAGGCCAACTTATAGGCGTTGGTGAACTCCTATTC...         12      coding   \n",
            "4  GGGGGCCAATCCGTCATCGCAGGTACCCGCTTCGTGTGCAGACCTG...          X  non-coding   \n",
            "\n",
            "   sequence_length  ancestry phenotype  \n",
            "0               50  European  diseased  \n",
            "1              119   African  diseased  \n",
            "2               96  European   healthy  \n",
            "3               53     Asian   healthy  \n",
            "4              130   African  diseased  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Define DNA nucleotides\n",
        "nucleotides = ['A', 'T', 'C', 'G']\n",
        "\n",
        "# Function to generate a random DNA sequence of a given length\n",
        "def generate_dna_sequence(length):\n",
        "    return ''.join(random.choices(nucleotides, k=length))\n",
        "\n",
        "# Create a synthetic dataset\n",
        "def create_synthetic_dataset(n_samples=1000):\n",
        "    data = []\n",
        "    for _ in range(n_samples):\n",
        "        sample_id = fake.uuid4()\n",
        "        sequence_length = random.randint(50, 150)\n",
        "        sequence = generate_dna_sequence(sequence_length)\n",
        "        chromosome = random.choice(list(range(1, 23)) + ['X', 'Y'])\n",
        "        region_type = random.choice(['coding', 'non-coding'])\n",
        "        ancestry = random.choice(['African', 'Asian', 'European', 'American'])\n",
        "        phenotype = random.choice(['healthy', 'diseased'])\n",
        "        data.append([sample_id, sequence, chromosome, region_type, sequence_length, ancestry, phenotype])\n",
        "    return pd.DataFrame(data, columns=['sample_id', 'sequence', 'chromosome', 'region_type', 'sequence_length', 'ancestry', 'phenotype'])\n",
        "\n",
        "# Create the dataset\n",
        "df = create_synthetic_dataset()\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "Hp4Jo7JVPNTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode categorical columns\n",
        "label_encoders = {}\n",
        "for column in ['chromosome', 'region_type', 'ancestry', 'phenotype']:\n",
        "    le = LabelEncoder()\n",
        "    # Convert the column to string type before encoding to ensure uniformity\n",
        "    df[column] = df[column].astype(str)  # This line is added to fix the error\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df[['sequence_length', 'chromosome', 'region_type', 'ancestry']].values\n",
        "y = df['phenotype'].values\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MT7BsgKnJGbM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concrete.ml.sklearn import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features to have zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Now we train in the clear and quantize the weights\n",
        "model = LogisticRegression(n_bits=8)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# We can simulate the predictions in the clear\n",
        "y_pred_clear = model.predict(X_test)\n",
        "\n",
        "# We then compile on a representative set\n",
        "model.compile(X_train)\n",
        "\n",
        "# Finally we run the inference on encrypted inputs !\n",
        "y_pred_fhe = model.predict(X_test, fhe=\"execute\")\n",
        "\n",
        "print(\"In clear  :\", y_pred_clear)\n",
        "print(\"In FHE    :\", y_pred_fhe)\n",
        "print(f\"Similarity: {int((y_pred_fhe == y_pred_clear).mean()*100)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VyrktibKnj7",
        "outputId": "73502c19-2755-43a5-9f5b-301f68fbe7c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In clear  : [1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0]\n",
            "In FHE    : [1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0]\n",
            "Similarity: 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overhead Analysis**"
      ],
      "metadata": {
        "id": "YKcfcdHWPWop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concrete.ml.sklearn import LogisticRegression\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Define DNA nucleotides\n",
        "nucleotides = ['A', 'T', 'C', 'G']\n",
        "\n",
        "# Function to generate a random DNA sequence of a given length\n",
        "def generate_dna_sequence(length):\n",
        "    return ''.join(random.choices(nucleotides, k=length))\n",
        "\n",
        "# Create a synthetic dataset\n",
        "def create_synthetic_dataset(n_samples=100000):  # Adjust sample size as needed\n",
        "    data = []\n",
        "    for _ in range(n_samples):\n",
        "        sample_id = fake.uuid4()\n",
        "        sequence_length = random.randint(50, 150)\n",
        "        sequence = generate_dna_sequence(sequence_length)\n",
        "        chromosome = random.choice(list(range(1, 23)) + ['X', 'Y'])\n",
        "        region_type = random.choice(['coding', 'non-coding'])\n",
        "        ancestry = random.choice(['African', 'Asian', 'European', 'American'])\n",
        "        phenotype = random.choice(['healthy', 'diseased'])\n",
        "        data.append([sample_id, sequence, chromosome, region_type, sequence_length, ancestry, phenotype])\n",
        "    return pd.DataFrame(data, columns=['sample_id', 'sequence', 'chromosome', 'region_type', 'sequence_length', 'ancestry', 'phenotype'])\n",
        "\n",
        "# Create the dataset\n",
        "df = create_synthetic_dataset()\n",
        "\n",
        "# Encode categorical columns\n",
        "label_encoders = {}\n",
        "for column in ['chromosome', 'region_type', 'ancestry', 'phenotype']:\n",
        "    le = LabelEncoder()\n",
        "    df[column] = df[column].astype(str)\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df[['sequence_length', 'chromosome', 'region_type', 'ancestry']].values\n",
        "y = df['phenotype'].values\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features to have zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train in the clear and quantize the weights\n",
        "model = LogisticRegression(n_bits=8)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Compile the model for FHE execution\n",
        "model.compile(X_train)\n",
        "\n",
        "# Function to test predictions for a specific batch size\n",
        "def test_predictions(batch_size):\n",
        "    # Prepare batched data\n",
        "    X_test_batch = np.repeat(X_test, batch_size // len(X_test) + 1, axis=0)[:batch_size]\n",
        "    y_test_batch = np.repeat(y_test, batch_size // len(y_test) + 1, axis=0)[:batch_size]\n",
        "\n",
        "    # Measure in-clear predictions\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    y_pred_clear = model.predict(X_test_batch)\n",
        "    end_time = time.time()\n",
        "    peak_memory_clear = tracemalloc.get_traced_memory()[1] / 1e3  # Convert bytes to KB\n",
        "    tracemalloc.stop()\n",
        "    clear_time = end_time - start_time\n",
        "\n",
        "    # Measure FHE predictions\n",
        "    tracemalloc.start()\n",
        "    start_time_fhe = time.time()\n",
        "    y_pred_fhe = model.predict(X_test_batch, fhe=\"execute\")\n",
        "    end_time_fhe = time.time()\n",
        "    peak_memory_fhe = tracemalloc.get_traced_memory()[1] / 1e3  # Convert bytes to KB\n",
        "    tracemalloc.stop()\n",
        "    fhe_time = end_time_fhe - start_time_fhe\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity = (y_pred_fhe == y_pred_clear).mean() * 100  # Percentage similarity\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        \"Batch Size\": batch_size,\n",
        "        \"In-Clear Time (s)\": clear_time,\n",
        "        \"FHE Time (s)\": fhe_time,\n",
        "        \"In-Clear Peak Memory (KB)\": peak_memory_clear,\n",
        "        \"FHE Peak Memory (KB)\": peak_memory_fhe,\n",
        "        \"Similarity (%)\": similarity,\n",
        "    }\n",
        "\n",
        "# Test for large batch sizes\n",
        "batch_sizes = [10, 100, 1000, 10000, 100000]\n",
        "results = [test_predictions(batch_size) for batch_size in batch_sizes]\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4xsWr5X-vp7",
        "outputId": "6a782b39-bca4-4664-d517-a4656680abc3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Batch Size  In-Clear Time (s)  FHE Time (s)  In-Clear Peak Memory (KB)  \\\n",
            "0          10           0.000739      0.059212                      2.296   \n",
            "1         100           0.000509      0.571410                      9.496   \n",
            "2        1000           0.000937      6.073873                     81.496   \n",
            "3       10000           0.001729     62.592004                    640.744   \n",
            "4      100000           0.005954    634.109776                   6400.744   \n",
            "\n",
            "   FHE Peak Memory (KB)  Similarity (%)  \n",
            "0                13.226           100.0  \n",
            "1                48.727           100.0  \n",
            "2               444.605           100.0  \n",
            "3              3386.499           100.0  \n",
            "4             33245.416           100.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def generate_selected_figures(results_df):\n",
        "    # Extract batch sizes and metrics from the DataFrame\n",
        "    batch_sizes = results_df[\"Batch Size\"]\n",
        "    in_clear_time = results_df[\"In-Clear Time (s)\"]\n",
        "    fhe_time = results_df[\"FHE Time (s)\"]\n",
        "    in_clear_memory = results_df[\"In-Clear Peak Memory (KB)\"]\n",
        "    fhe_memory = results_df[\"FHE Peak Memory (KB)\"]\n",
        "\n",
        "    # Calculate overhead ratios\n",
        "    time_overhead_ratio = fhe_time / in_clear_time\n",
        "    memory_overhead_ratio = fhe_memory / in_clear_memory\n",
        "\n",
        "    # Batch Size vs FHE Prediction Time\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, fhe_time, marker='o', label='FHE Time', color='red')\n",
        "    plt.plot(batch_sizes, in_clear_time, marker='o', label='In-Clear Time', color='blue')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Batch Size (log scale)')\n",
        "    plt.ylabel('Prediction Time (s)')\n",
        "    plt.title('Batch Size vs Prediction Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "    plt.savefig('batch_size_vs_prediction_time.png', bbox_inches='tight')  # Save with no extra white space\n",
        "    plt.close()\n",
        "\n",
        "    # Batch Size vs FHE Memory Usage\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, fhe_memory, marker='o', label='FHE Memory Usage', color='red')\n",
        "    plt.plot(batch_sizes, in_clear_memory, marker='o', label='In-Clear Memory Usage', color='blue')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Batch Size (log scale)')\n",
        "    plt.ylabel('Memory Usage (KB)')\n",
        "    plt.title('Batch Size vs Memory Usage')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "    plt.savefig('batch_size_vs_memory_usage.png', bbox_inches='tight')  # Save with no extra white space\n",
        "    plt.close()\n",
        "\n",
        "    # Time Overhead Ratio vs Batch Size\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, time_overhead_ratio, marker='o', label='Time Overhead Ratio (FHE/In-Clear)', color='green')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Batch Size (log scale)')\n",
        "    plt.ylabel('Overhead Ratio')\n",
        "    plt.title('Time Overhead Ratio vs Batch Size')\n",
        "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "    plt.legend()\n",
        "    plt.savefig('time_overhead_ratio_vs_batch_size.png', bbox_inches='tight')  # Save with no extra white space\n",
        "    plt.close()\n",
        "\n",
        "    # Memory Overhead Ratio vs Batch Size\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, memory_overhead_ratio, marker='o', label='Memory Overhead Ratio (FHE/In-Clear)', color='purple')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Batch Size (log scale)')\n",
        "    plt.ylabel('Overhead Ratio')\n",
        "    plt.title('Memory Overhead Ratio vs Batch Size')\n",
        "    plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "    plt.legend()\n",
        "    plt.savefig('memory_overhead_ratio_vs_batch_size.png', bbox_inches='tight')  # Save with no extra white space\n",
        "    plt.close()\n",
        "\n",
        "# Generate the figures\n",
        "generate_selected_figures(results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "Yc77DsOkGWa2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxLjzswzJ0_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}